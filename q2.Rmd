```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
### Question 2

```{r}
library(tidyverse)
library(mvtnorm) #used for randomly generating data from multivariate normal
library(factoextra)
library(gridExtra)
library(corrplot)
library(knitr)
### Scale the quantitative Variables
```

```{r}
ProjQuant <- 
  Proj %>%
  select(G80, vLTmax, vCO, Fmax, FD, FO)

PQScale <- as.data.frame(scale(x = ProjQuant, center = TRUE, scale = TRUE))

```
### Check Corrrelation and Outliers
```{r}
cor(PQScale)
corrplot(cor(PQScale))

PQScale_long <- PQScale %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

ggplot(PQScale_long, aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "yellow") +
  labs(title = "Boxplots of Scaled Thermophysical Variables")

#Finding the outliers
# Example: find outliers in FO
boxplot.stats(PQScale$FO)$out

# Find which rows they belong to
which(PQScale$FO %in% boxplot.stats(PQScale$FO)$out)

```


### Create elbow plot
```{r}
set.seed(123)
fviz_nbclust(x = PQScale,
             FUNcluster = kmeans,
             method = "wss",
             k.max = 10,
             nstart = 25,
             iter.max = 25) +
  labs(subtitle = "K-means")
```

### Part c. - Perform K-means with K = 3 and extract Total Within Cluster Variation

```{r}
set.seed(123)
kmeansPQScale <- kmeans(x = PQScale, centers = 3, iter.max = 25, nstart = 15)
set.seed(NULL)

kmeansPQScale$tot.withinss
```

Based on kmeans with 3 clusters, the following is the total within-cluster variation: 250.4097

### How many in each cluster?

```{r}
table(kmeansPQScale$cluster)
```

Based on kmeans with 5 clusters, the followoing is the number of observations assigned with each cluster: 

### Part e. - Perform PCA and create visualization for K = 3 (K-means)
```{r}
#Using prcomp to use and standardize the data
ProjPCA <- prcomp(x = PQScale, center = TRUE, scale. = TRUE)

#Calculate PVE (used in the next steps)
PVE <- ProjPCA$sdev^2 / sum(ProjPCA$sdev^2)

#Create the df for the plots
Projtemp_df <- as.data.frame(x = ProjPCA$x[ , 1:2])

#Add cluster assignment to the dataset
Projtemp_df <- 
  Projtemp_df %>%
  mutate(cluster = kmeansPQScale$cluster)

#Create Plot
ggplot(data = Projtemp_df, mapping = aes(x = PC1, y = PC2,
                                  color = as.factor(cluster),
                                  shape = as.factor(cluster))) + 
  geom_point() + 
  labs(x = paste("PC1 (", round(100*PVE[1], digits = 1), "%)", sep = ""),
       y = paste("PC2 (", round(100*PVE[2], digits = 1), "%)", sep = ""),
       color = "Cluster Assignment",
       shape = "Cluster Assignment")
```


## Heirarchical Clustering
### Part a - Perform hierarchical clustering using Euclidean distance and complete linkage
```{r}
eucDistProj <- stats::dist(x = PQScale, method = "euclidean")
hcComp <- hclust(d = eucDistProj, method = "complete")

fviz_nbclust(x = PQScale, FUNcluster = hcut, 
             method = "wss", 
             k.max = 10,
             hc_func = "hclust", 
             hc_metric = "euclidean",
             hc_method = "complete") +
  labs(subtitle = "Complete Linkage")
```


## Heirarchical clustering
```{r}
  fviz_dend(x = hcComp, k = 3, rect = TRUE) +
  labs(subtitle = "Complete Linkage K = 3") +
  theme(legend.position = "none")
```

### Part d. - Perform PCA and create visualization for K = 3 (Hierarchical)
```{r}
#Extract cluster assignments
hcCompClust <- cutree(tree = hcComp, k = 3)

hcCompClustdf <- as.data.frame(hcCompClust)


#Using prcomp to use and standardize the data
ProjPCA <- prcomp(x = PQScale, center = TRUE, scale. = TRUE)

#Calculate PVE (used in the next steps)
PVE <- ProjPCA$sdev^2 / sum(ProjPCA$sdev^2)

#Create the df for the plots
Projtemp_dfQ3 <- as.data.frame(x = ProjPCA$x[ , 1:2])

#Add cluster assignment to the dataset
Projtemp_dfQ3 <- 
  Projtemp_dfQ3 %>%
  mutate(cluster = hcCompClustdf$hcCompClust)

#Create Plot
ggplot(data = Projtemp_dfQ3, mapping = aes(x = PC1, y = PC2,
                                  color = as.factor(cluster),
                                  shape = as.factor(cluster))) + 
  geom_point() + 
  labs(x = paste("PC1 (", round(100*PVE[1], digits = 1), "%)", sep = ""),
       y = paste("PC2 (", round(100*PVE[2], digits = 1), "%)", sep = ""),
       color = "Cluster Assignment",
       shape = "Cluster Assignment")
```

### Add the cluster assignments to the dataset
```{r}
# Add k-means cluster assignment to original dataset
Proj_with_kmeans <- 
  Proj %>%
  mutate(kmeans_cluster = kmeansPQScale$cluster)
# Add hierarchical cluster assignment to original dataset
Proj_with_clusters <- 
  Proj_with_kmeans %>%
  mutate(hierarchical_cluster = hcCompClust)
# View the first few rows with both cluster assignments

Proj_with_clusters <- Proj_with_clusters %>%
  mutate(kmeans_cluster_matched = case_when(
    kmeans_cluster == 3 ~ 1,
    kmeans_cluster == 1 ~ 3,
    TRUE ~ kmeans_cluster  # leaves 2 unchanged
  ))

Proj_with_clusters <- Proj_with_clusters %>%
  select(-kmeans_cluster)

Proj_with_clusters <- Proj_with_clusters %>%
  mutate(cluster_match = if_else(kmeans_cluster_matched == hierarchical_cluster, "Match", "No Match"))


print(Proj_with_clusters)

cluster_comparison_df <- Proj_with_clusters %>%
  select(ID, kmeans_cluster_matched, hierarchical_cluster, cluster_match)

print(cluster_comparison_df)
```


```{r}
Proj_with_clusters %>%
  group_by(kmeans_cluster_matched) %>%
  summarise(across(c(G80, vLTmax, vCO, Fmax, FD, FO), mean, .names = "mean_{.col}"))

```

From the clusters, we can identify the clusters have the following traits:
Cluster 1: High firmness and low flow
Cluster 2: Moderate Firmness, lower oil, and meltability
Cluster 3: Soft and oily, high flow.
